apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-thrift-deployment
  labels:
    app: spark-thrift
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-thrift-driver
  template:
    metadata:
      labels:
        app: spark-thrift-driver
    spec:
      initContainers:
      - name: block-cloud-metadata
        image:  jupyterhub/k8s-network-tools:0.8.2
        command: ["iptables", "-A", "OUTPUT", "-d", "169.254.169.254", "-j", "DROP"]
        securityContext:
            privileged: true
            runAsUser: 0
            capabilities: 
              add: ["NET_ADMIN"]
      containers:
      - name: spark-thrift-axs
        image: ctslater/spark-axs-hive
        ports:
        - containerPort: 10000
        - containerPort: 7123
        args: ["/opt/spark/sbin/start-thriftserver.sh", "--master", "k8s://https://kubernetes.default.svc", "--deploy-mode", "client",
                "--name", "spark-sql",
                "--conf", "spark.executor.instances=2",
                # Enables JDB debugging. Can be removed when no longer needed.
                "--conf", "spark.driver.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,address=5000,server=y,suspend=n",
                "--conf", "spark.kubernetes.namespace=ctslater",
                "--conf", "spark.kubernetes.driver.pod.name=$(DRIVER_POD_NAME)",
                "--conf", "spark.driver.host=spark-thrift-driver-svc",
                "--conf", "spark.driver.port=4444",
                "--conf", "spark.driver.bindAddress=0.0.0.0",
                # Other credential providers like STSAssumeRoleWithWebIdentitySessionCredentialsProvider
                # don't have the right API; the default chain seems to make up for those.
                "--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain",
                # This takes a random port if not specified; need to expose it through thrift-driver-svc
                "--conf", "spark.blockManager.port=7123",
                # This sharedPrefixes setting is critical, otherwise there will be errors like
                # "Class class com.amazonaws.auth.com.amazonaws.auth.DefaultAWSCredentialsProviderChain does not
                # implement AWSCredentialsProvider"
                # See https://www.opencore.com/blog/2018/3/spark-shared-prefixes/
                "--conf", "spark.sql.hive.metastore.sharedPrefixes=com.amazonaws.",
                "--conf", "spark.kubernetes.executor.podTemplateFile=/opt/spark/templates/spark-exec-template.yaml",
                "--conf", "spark.kubernetes.container.image=ctslater/spark-axs-hive"
                ]
        env:
        - name: SPARK_NO_DAEMONIZE
          value: "1"
        - name: AWS_REGION
          value: "us-west-2"
        - name: DRIVER_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: work-dir
          mountPath: /opt/spark/work-dir
        - name: logs
          mountPath: /opt/spark/logs/
        - name: ivycache
          mountPath: /opt/spark/.ivy2
        - name: awscache
          mountPath: /opt/spark/.aws
        - name: exectemplate
          mountPath: /opt/spark/templates/
        - name: schemas
          mountPath: /opt/spark/schemas/
      # This pod needs extra permissions to start the executor pods
      serviceAccountName: thrift-sa
      securityContext:
        # Need to specify this so the AWS token file is readable by non-root.
        fsGroup: 1337
      volumes:
      - name: work-dir
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: ivycache
        emptyDir: {}
      - name: awscache
        emptyDir: {}
      - name: exectemplate
        configMap:
          name: exectemplate
          items:
            - key: spark-exec-template.yaml
              path: spark-exec-template.yaml
      - name: schemas
        configMap:
          name: tap-schema-s3
          items:
          - key: tap_schema_s3.hivesql
            path: tap_schema_s3.hivesql
          - key: ztf_oct19.hivesql
            path: ztf_oct19.hivesql
